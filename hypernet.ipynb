{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERNETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTIOIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypernetworks, or hypernets in short, are neural networks that generate weights for another neural network, known as the target network. \n",
    "(They have emerged as a powerful deep learning technique that allows for greater flexibility, adaptability, dynamism, faster training, information sharing, and model compression etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Deep learning (DL) has revolutionized the field of artificial intelligence by enabling remarkable advancements in various\n",
    "domains, including computer vision [12], natural language processing [17], causal inference [11], and reinforcement\n",
    "learning [34] etc.)\n",
    "(However, despite their success, standard DNNs remain restrictive in certain conditions.\n",
    "For example, once a DNN is trained, its weights as well as architecture are fixed, e.g., [54, 72], and any changes\n",
    "to weights or architecture need re-training the DNN)\n",
    "(. This lack of adaptability and dynamism restrict the flexibility\n",
    "of the DNNs, making them less suitable for scenarios where dynamic adjustments or data-adaptivity are required)\n",
    "Hypernetworks (or hypernets in short) have emerged as a promising architectural paradigm to enhance the flexibility\n",
    "(through data-adaptivity and dynamic architectures) and performance of DNNs.\n",
    " Hypernets are a class of neural networks\n",
    "that generate the weights/parameters of another neural network called target/main/primary network, where both the\n",
    "networks are trained in an end-to-end differentiable manner [23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key characteristics and advantages of hypernets that offer applications across different problem settings are discussed below.\n",
    "* Soft weight sharing: Hypernetworks can be trained to generate the weights of multiple DNNs for solving related tasks [14, 48]. This is called soft weight sharing as unlike hard weight sharing that involves shared layers among tasks (e.g., in multitasking), here different DNNs are generated by a common hypernet through task-conditioning. This is helpful for sharing information among tasks and can be used for transfer learning or dynamic information sharing [14].\n",
    "* Dynamic architectures: Hypernetworks can be used to generate the weights of a network with a dynamic architecture, where the number of layers or the structure of the network changes during training or inference. This can be particularly useful for tasks where the target network structure is not known at training time [23].\n",
    "* Data-adaptive DNNs: Unlike the standard DNNs whose weights are fixed at inference time, HyperDNNs can be developed to generate target network customized to the needs of the data. In such cases, hypernets are conditioned on the input data to adapt to the data [68].\n",
    "* Uncertainty quantification: Hypernets can effectively train uncertainty aware DNNs by leveraging techniques\n",
    "like sampling multiple inputs from the noise distribution [32] or incorporating dropout within the hypernets\n",
    "themselves [14]. By generating multiple sets of weights for the main network, hypernets create an ensemble\n",
    "of models, each with different parameter configurations. This ensemble-based approach aids in estimating\n",
    "uncertainty in the model predictions, a crucial aspect for safety-critical applications like healthcare, where\n",
    "having a measure of confidence in predictions is essential\n",
    "*  Parameter efficiency: HyperDNNs, i.e., DNNs trained with hypernets can have fewer weights than the\n",
    "corresponding standard DNNs, resulting in weight compression [79]. This can be particularly useful when\n",
    "working with limited resources, limited data or when dealing with high-dimensional data and can result in\n",
    "faster training than the corresponding DNN [44]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERNETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTIOIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hipernety lub w skrócie hipernety to Sieci neuronowe, które generują wagi dla innej sieci neuronowej, znanej jako sieć docelowa.\n",
    "(Pojawiły się jako potężna technika głębokiego uczenia, która pozwala na większą elastyczność, zdolność adaptacji, dynamikę, szybsze szkolenie, wymianę informacji i kompresję modelu itp.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Deep learning (DL) zrewolucjonizował dziedzinę sztucznej inteligencji umożliwiając niezwykłe postępy w różnych dzidzinach, w tym wizja komputerowa, przetwarzanie języka naturalnego, wnioskowanie przyczynowe, i reinforcement learningu itp.)\n",
    "\n",
    "\n",
    "(Jednak pomimo ich sukcesu standardowe DNN pozostają restrykcyjne w pewnych warunkach. Na przykład, gdy DNN jest pzetrenowany, jego wagi, a także architektura są stałe i wszelkie zmiany w wagach lub architekturze wymagają ponownego przetrenowania DNN) \n",
    "\n",
    "\n",
    "(Ten brak zdolności adaptacyjnych i dynamizmu ogranicza elastyczność\n",
    "DNN, co czyni je mniej odpowiednimi do scenariuszy, w których wymagane są dynamiczne korekty lub adaptacja danych)\n",
    "\n",
    "\n",
    "\n",
    "(Hypernetworks pojawiły się jako obiecujący paradygmat architektoniczny w celu zwiększenia elastyczności poprzez adaptacyjność danych i architektury dynamicznie oraz wydajność DNN. Hipernety to klasa sieci neuronowych\n",
    "które generują wagi innej sieci neuronowej zwanej siecią docelową/główną/podstawową, gdzie zarówno sieci są szkolone w sposób end-to-end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kluczowe cechy i zalety hipernetów, które oferują aplikacje w różnych ustawieniach problemów:\n",
    "\n",
    "* Soft weight sharing: Hipernetworks można trenować do generowania wag wielu DNN do rozwiązywania powiązanych zadań. Nazywa się to soft weight sharing, ponieważ w przeciwieństwie do hard weight sharing, który obejmuje wspólne warstwy między zadaniami (np. w multitasking), tutaj różne DNN są generowane przez wspólną hipernetę poprzez warunkowanie zadań. Jest to pomocne przy dzieleniu się informacjami między zadaniami i może być wykorzystywane do uczenia się transferowego lub dynamicznego udostępniania informacji.\n",
    "\n",
    "\n",
    "* Dynamic architectures: Hipernetworks mogą być używane do generowania wag sieci o architekturze dynamicznej, w której liczba warstw lub struktura sieci zmienia się podczas szkolenia lub wnioskowania. Może to być szczególnie przydatne w przypadku zadań, w których docelowa struktura sieci nie jest znana w czasie szkolenia.\n",
    "\n",
    "\n",
    "* Data-adaptive DNN: w przeciwieństwie do standardowego DNN, którego wagi są ustalane w czasie wnioskowania, HyperDNN można opracować w celu generowania sieci docelowej dostosowanej do potrzeb danych. W takich przypadkach hipernety są uwarunkowane danymi wejściowymi w celu dostosowania się do danych.\n",
    "\n",
    "\n",
    "* Uncertainty quantification: Hipernety mogą skutecznie trenować DNN ktore świadome o niepewności, wykorzystując techniki jak multiple inputs z rozkładu szumów lub włączenie przerywania w hipernetach sami. Generując wiele zestawów wag dla sieci głównej, hipernety tworzą zespół modeli, każdy z różnymi konfiguracjami parametrów. To podejście oparte na zespole pomaga w szacowaniu niepewność w przewidywaniach modelu, kluczowy aspekt dla zastosowań krytycznych dla bezpieczeństwa, takich jak opieka zdrowotna, gdzie niezbędna jest miara zaufania do prognoz\n",
    "\n",
    "\n",
    "* Parameter efficiency: DNN przeszkolony w hipernetach może mieć mniej wag niż\n",
    "standardowe DNNs, powodujące z kompresji wag. Może to być szczególnie przydatne, gdy\n",
    "cos diziala z ograniczonymi zasobami, ograniczonymi danymi lub w przypadku danych o dużych wymiarach i może skutkować szybszy trening niż zlykle DNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struktura i rola: Main Network to typowa sieć neuronowa, którą można zaprojektować do dowolnego zadania głębokiego uczenia, takiego jak klasyfikacja obrazu, przetwarzanie języka naturalnego lub inne zadania. Struktura tej sieci może się różnić w zależności od konkretnego zadania: może to być splotowa sieć neuronowa (CNN) do przetwarzania obrazu, powtarzająca się sieć neuronowa (RNN) do przetwarzania sekwencji danych i tak dalej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uczenie: w normalnej sytuacji wagi tej sieci są inicjowane losowo, a następnie optymalizowane w procesie uczenia się, aby rozwiązać dany problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypernetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struktura i rola: Hypernetwork to oddzielna sieć neuronowa, której celem jest generowanie wag dla sieci głównej. Ta sieć jest zwykle mniejsza i może być zaprojektowana tak, aby uwzględniać pewne cechy zadania, do którego szkolona jest sieć główna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interakcja z siecią główną: Hypernetwork pobiera dane wejściowe, które mogą zawierać informacje o stanie lub strukturze sieci głównej i na podstawie tych danych generuje wagi dla sieci głównej. Oznacza to, że wagi głównej sieci nie są bezpośrednio optymalizowane za pomocą standardowych metod szkoleniowych, takich jak propagacja wsteczna błędu, ale są generowane i zmieniane dynamicznie dzięki Hypernetwork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorisation of Hypernetworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует разделение на 5 критериев дезайна гиперсетей\n",
    "- input-based \n",
    "- output-based\n",
    "- variability of inputs \n",
    "- variability of outputs\n",
    "- architecture-based\n",
    "\n",
    "с помощью каждого из них можно можно подробнее понять работы конкретной гиперсети отвечая на вопросы:\n",
    "\n",
    "![categorisation.png](./imgs/categorisation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input-based hypernetworks\n",
    "\n",
    "Hypernetworkш przyjmują wektor kontekstowy jako dane wejściowe i generują wagi docelowej sieci DNN jako wyjście. W zależności od użytego wektora kontekstowego możemy wyróżnić następujące typy hypernetworks:\n",
    "\n",
    "\n",
    "- **Task-conditioned hypernetworks**: Te sieci przyjmują informacje specyficzne dla zadania jako dane wejściowe, takie jak tożsamość zadania, hiperparametry, architektury lub inne wskazówki specyficzne dla zadania. Sieć generuje wagi dostosowane do konkretnego zadania, co pozwala na dostosowanie jej zachowania i współdzielenie informacji między zadaniami, co skutkuje lepszą wydajnością.\n",
    "\n",
    "\n",
    "- **Data-conditioned hypernetworks**: Te sieci są warunkowane danymi, na których szkolona jest docelowa sieć. Sieć generuje wagi na podstawie charakterystyki danych wejściowych, co pozwala na dynamiczną adaptację modelu do konkretnych wzorców lub cech, prowadząc do większej elastyczności i lepszej generalizacji.\n",
    "\n",
    "\n",
    "- **Noise-conditioned hypernetworks**: Te sieci nie są warunkowane żadnymi danymi wejściowymi ani wskazówkami zadania, a raczej losowo wybranym szumem. Sprawia to, że są bardziej uniwersalne i pomagają w kwantyfikacji niepewności predykcyjnej dla DNN, ale mogą nie osiągać tak dobrych wyników jak sieci warunkowane zadaniem lub danymi na wielu zadaniach lub zbiorach danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output-based Hypernetworks\n",
    "\n",
    "W zależności od sposobu generowania wag, czyli strategii generacji wag, klasyfikujemy hypernetworki na te, które generują wszystkie wagi naraz, i te, które tego nie robią. Klasyfikacja ta jest ważna, ponieważ kontroluje skalowalność i złożoność hypernetworks, ponieważ typowe DNN mają dużą liczbę wag, a ich jednoczesna produkcja może powiększyć rozmiar ostatniej warstwy hypernetworki.\n",
    "\n",
    "- **Generate Once**: Te sieci generują wagi całej docelowej sieci DNN naraz. Wszystkie wygenerowane wagi są używane, a wagi każdej warstwy są generowane razem, w przeciwieństwie do innych strategii generowania wag. Jednakże, ta metoda generowania wag nie jest odpowiednia dla dużych sieci docelowych, ponieważ może to prowadzić do skomplikowanych hypernetwork.\n",
    "\n",
    "- **Generate Multiple**: Te sieci mają wiele głów????? do produkcji wag (czasami nazywane jako split/multi-head hypernetwork) i ta metoda generowania wag może uzupełniać inne podejścia. Upraszcza to złożoność i redukuje liczbę wag wymaganych w ostatniej warstwie hypernetwork przez liczbę głów. Ta metoda nie wymaga dodatkowych osadzeń i ogólnie używa wszystkich wygenerowanych wag, w przeciwieństwie do podejść generowania wag component-wise i chunk-wise, gdzie niektóre wagi pozostają nieużywane.\n",
    "\n",
    "\n",
    "- **Generate Chunk-wise**: Sieci tego typu generują wagi docelowej sieci w częściach. Może to prowadzić do niewykorzystania niektórych wygenerowanych wag, ponieważ są one produkowane zgodnie z rozmiarem części, który może nie odpowiadać rozmiarom warstw. Jeśli rozmiar części jest mniejszy niż warstwa, wagi dla całej warstwy mogą nie być generowane jednocześnie. Ponadto, te sieci wymagają dodatkowych osadzeń do rozróżniania różnych części i generowania specyficznych wag dla nich. Jednak ogólnie generowanie wag w częściach prowadzi do zmniejszenia złożoności i poprawy skalowalności sieci.\n",
    "\n",
    "- **Generate Component-wise**: Strategia generowania wag składowych polega na oddzielnym tworzeniu wag dla każdego indywidualnego komponentu (takiego jak warstwa lub kanał) modelu docelowego. Jest to pomocne w generowaniu specyficznych wag, ponieważ różne warstwy lub kanały reprezentują różne cechy lub wzorce w sieci. Jednak podobnie jak w podejściu kawałkowym, sieci hypernetwork składowe wymagają osadzenia dla każdego komponentu, aby odróżnić różne komponenty i wytworzyć wagi specyficzne dla danego komponentu. Pomaga to również zmniejszyć złożoność i poprawić skalowalność sieci hypernetwork. Ponieważ wagi są generowane zgodnie z rozmiarem największej warstwy, ten sposób generowania wag może prowadzić do niewykorzystania niektórych wag w mniejszych warstwach. Ta strategia może być postrzegana jako specjalny przypadek podejścia kawałkowego do generowania wag, gdzie jeden kawałek odpowiada rozmiarowi jednego komponentu.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variability of Inputs\n",
    "\n",
    "We can categorize hypernets based on the variability of the inputs. We have two classes, static inputs and dynamic\n",
    "inputs, as discussed below.\n",
    "\n",
    "- Static Inputs: If the inputs are predefined and are fixed then the hypernet is called static with respect to the inputs. For\n",
    "example, multitasking [42] has fixed number of tasks leading to fixed number of inputs. It is to be noted that here fixed\n",
    "input only means fixed tasks identities, however hypernets can learn embeddings for different tasks.\n",
    "\n",
    "\n",
    "- Dynamic Inputs: If the inputs change and generally are dependent on data on which the target network is trained,\n",
    "then the hypernet is called dynamic with respect to the inputs. Dynamic inputs help hypernetworks to introduce a new\n",
    "level of adaptability by dynamically generating the weights of the target network. This dynamic weight generation\n",
    "enables hypernetworks to respond to input-dependent context and adjust their behavior accordingly. By generating\n",
    "network weights based on specific inputs, hypernetworks can capture intricate patterns and dependencies that may vary\n",
    "across different instances of data. This adaptability leads to enhanced model performance, especially in scenarios with\n",
    "complex and evolving data distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variability of Outputs\n",
    "\n",
    "Klasyfikując sieci hypernetwork na podstawie charakterystyki wag sieci docelowej, możemy podzielić je na dwa typy: statyczne i dynamiczne wyjścia.\n",
    "\n",
    "\n",
    "- **Static Outputs**: Jeśli wagi sieci docelowej mają stały rozmiar, to sieć hypernetwork nazywana jest statyczną względem wyjść. W takim przypadku sieć docelowa również jest statyczna.\n",
    "\n",
    "\n",
    "- **Dynamic Outputs**: Jeśli wagi sieci docelowej nie są stałe, architektura zmienia się w zależności od rozmiaru, to sieć hypernetworku nazywana jest dynamiczną względem wyjść, a sieć docelowa również jest siecią dynamiczną, ponieważ może mieć różną architekturę w zależności od wejścia sieci hypernetwork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamism in Hypernetworks\n",
    "\n",
    "- **Static Hypernets**: Jeśli wejście sieci hypernetwork jest stałe, tj. z góry określone, a liczba wag generowanych przez sieć hypernetwork dla sieci docelowej jest stała, tj. architektura jest stała, wówczas sieć hypernetwork nazywana jest statyczną. Takie sieci pracują z z góry określonymi wejściami, np. tożsamościami zadań, które mogą być uczone jako osadzenia, ale rozwiązywane zadania pozostają te same.\n",
    "\n",
    "\n",
    "\n",
    "- **Dynamic Hypernets**: Jeśli wejście sieci hypernetwork opiera się na danych wejściowych sieci docelowej, tj. danych wejściowych, lub liczba wag generowanych przez sieć hypernetwork dla sieci docelowej jest zmienna, tj. architektura jest dynamiczna, wówczas sieć hypernetwork nazywana jest dynamiczną. Dynamiczne sieci hypernetwork są bardziej elastyczne i mogą dostosowywać swoje działanie i strukturę w odpowiedzi na zmieniające się warunki lub wymagania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of Hypernetworks\n",
    "\n",
    "W klasyfikacji sieci hypernetwork na podstawie ich architektury możemy wyróżnić cztery główne typy: wielowarstwowe perceptrony (MLP), konwolucyjne sieci neuronowe (CNN), rekurencyjne sieci neuronowe (RNN) oraz sieci oparte na mechanizmach uwagi.\n",
    "\n",
    "- **MLPs**: Sieci hypernetwork oparte na MLP wykorzystują gęstą i w pełni połączoną architekturę, co pozwala na kompleksowe generowanie wag, biorąc pod uwagę całą informację wejściową.\n",
    "\n",
    "\n",
    "- **CNNs**: Sieci hypernetwork oparte na CNN wykorzystują warstwy konwolucyjne do przechwytywania lokalnych wzorców i informacji przestrzennych, co sprawia, że są one skuteczne w zadaniach związanych z danymi przestrzennymi, takimi jak analiza obrazów czy wideo.\n",
    "\n",
    "\n",
    "- **RNNs**: Sieci hypernetwork oparte na RNN zawierają rekurencyjne połączenia w swojej architekturze, co umożliwia przetwarzanie informacji sekwencyjnych i dynamiczne generowanie wag na podstawie poprzednich stanów lub danych wejściowych, co czyni je odpowiednimi dla zadań związanych z danymi sekwencyjnymi.\n",
    "\n",
    "\n",
    "- **Attention-based hypernetworks**: Sieci hypernetwork oparte na mechanizmach uwagi selektywnie koncentrują się na istotnych cechach wejściowych, generując wagi dla sieci docelowej, co pozwala im uchwycić dalekosiężne zależności i poprawić jakość generowanych wyjść.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZASTOSOWANIA\n",
    "Hypernetworki zademonstrowały swoją użyteczność w szerokim zakresie zastosowań w deep learningu. \n",
    "* Continual Learning\n",
    "    * Model uczy się przez cały czas bez zapominania tego co został już nauczony.\n",
    "    * W przeciwieństwie do tradycyjnego uczenia na batchach gdzie ilośc danych jest stała, dane napływają cały czas w rosnącej ilosci.\n",
    "    * Modelując każdy dataset jako task i stosując task-conditioned hyperneta pozwoliło na współdzielenie informacji pomiedzy taskami\n",
    "    * Probem catastrophic forgettingu został rozwiązany za pomocą regularyzatora operujacego na wagach w ramach jednego taska\n",
    "    * (Huang, Y., Xie, K., Bharadhwaj, H., and Shkurti, F. (2021). Continual model-based reinforcement learning with hypernetworks)\n",
    "* Federated Learning\n",
    "    * Model trenowany jest na wielu urządzeniach jednocześnie \n",
    "    * Znajdujący się na centralnym serwerze hypernetwork odpowiada za aktualizowanie wag modelów\n",
    "    * Każde urządzenie jest reprezentowane jako pojedynczy task a hypernet stosuje task conditioning\n",
    "    * Shamsian, A., Navon, A., Fetaya, E., and Chechik, G. (2021). Personalized federated learning using hypernetworks. In International Conference on Machine Learning\n",
    "* Few-shot Learning\n",
    "    * Hypernetwork oparty na encoderze-dekoderze, który uczy zależną od danych, generatywną reprezentację parametrów modelu, który dzieli informacje między różnymi zadaniami poprzez miękkie wagi.\n",
    "    \n",
    "* Domain Adaptation\n",
    "    * Data conditioned hypernetworki gdzie przykłady z domen docelowych są używane jako dane wejściowe do hyperneru, który generuje\n",
    "    wagi dla sieci docelowej. Daje to hiper-sieciom możliwość uczenia się i dzielenia informacjami z istniejących domen z domeną docelową.\n",
    "    domeny docelowej poprzez wspólne uczenie.\n",
    "\n",
    "* Causal Inference\n",
    "    * apply hypernets to heterogeneous treatment effects\n",
    "    (HTE) estimation problem [14]. We applied task-conditioned hypernets where each potential outcome (PO) function\n",
    "    is considered as a task. Embeddings of PO functions are used as input to hypernet that generates parameters for the\n",
    "    corresponding PO function, i.e., factual and counterfactual models. Based on soft weight sharing of hypernets, this\n",
    "    work presents the first general mechanism to train HTE learners that enables end-to-end inter-treatment information\n",
    "    sharing among the PO functions and helps to get reliable estimates, especially with limited-size observational data. The\n",
    "    proposed framework also incorporates dropout in the hypernet that allows to generate multiple sets of parameters for\n",
    "    the PO functions and helps in uncertainty quantification\n",
    "\n",
    "* Uncertainty Quantification\n",
    "    * Hypernets can effectively train uncertainty aware DNNs by leveraging techniques\n",
    "    like sampling multiple inputs from the noise distribution [32] or incorporating dropout within the hypernets themselves\n",
    "    [14]. By generating multiple sets of weights for the main network, hypernets create an ensemble of models, each\n",
    "    with different parameter configurations. This ensemble-based approach aids in estimating uncertainty in the model\n",
    "    predictions. \n",
    "\n",
    "* Multitasking\n",
    "    *   Hypernety mogą być stosowane w kontekście multitasking w celu ułatwienia wspólnego\n",
    "        uczenia się wielu zadań poprzez dynamiczne generowanie lub dostosowywanie parametrów lub architektur modelu. W szczególności,\n",
    "        możemy trenować task conditioned hypernety, gdzie osadzanie tasku działa jako dane wejściowe do sieci, która\n",
    "        generuje wagi dla odpowiedniego zadania. Możemy albo wygenerować cały model dla każdego z zadań, albo tylko jego niewspółdzielone części.\n",
    "\n",
    "* Reinforcement Learning\n",
    "    * Hypernets can be used to dynamically generate or adapt network architectures, model parameters, or exploration strategies in RL agents. By\n",
    "    using a hypernetwork, the RL agent can effectively learn to customize its internal representations or policies based on\n",
    "    the specific characteristics of the environment or task. \n",
    "\n",
    "* Computer Vision\n",
    "    * Data conditioned hypernet gdzie obraz jest inputem do hypernetu do wyostrzania zdjęć\n",
    "    * Noise conditioned hypernet do klasfikacji obrazów\n",
    "    * Rozwiązania stosowane do problemów AutoML, Continual Learning i Federated Learning też mogą być zastosowane\n",
    "\n",
    "* Usprawnienie procesu uczenia modelów AutoML, Manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KIEDY UŻYĆ HYPERNETWORKA\n",
    "* Are there any related components in the problem setting under consideration?\n",
    "* Do we need a data-adaptive neural network?\n",
    "* Do we need a dynamic neural network architecture?\n",
    "* Do we need faster training/ parameter efficiency?\n",
    "* Do we need uncertainty quantification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRZYKŁADY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERNETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTIOIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hipernety lub w skrócie hipernety to Sieci neuronowe, które generują wagi dla innej sieci neuronowej, znanej jako sieć docelowa.\n",
    "(Pojawiły się jako potężna technika głębokiego uczenia, która pozwala na większą elastyczność, zdolność adaptacji, dynamikę, szybsze szkolenie, wymianę informacji i kompresję modelu itp.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Deep learning (DL) zrewolucjonizował dziedzinę sztucznej inteligencji umożliwiając niezwykłe postępy w różnych dzidzinach, w tym wizja komputerowa, przetwarzanie języka naturalnego, wnioskowanie przyczynowe, i reinforcement learningu itp.)\n",
    "\n",
    "\n",
    "(Jednak pomimo ich sukcesu standardowe DNN pozostają restrykcyjne w pewnych warunkach. Na przykład, gdy DNN jest pzetrenowany, jego wagi, a także architektura są stałe i wszelkie zmiany w wagach lub architekturze wymagają ponownego przetrenowania DNN) \n",
    "\n",
    "\n",
    "(Ten brak zdolności adaptacyjnych i dynamizmu ogranicza elastyczność\n",
    "DNN, co czyni je mniej odpowiednimi do scenariuszy, w których wymagane są dynamiczne korekty lub adaptacja danych)\n",
    "\n",
    "\n",
    "\n",
    "(Hypernetworks pojawiły się jako obiecujący paradygmat architektoniczny w celu zwiększenia elastyczności poprzez adaptacyjność danych i architektury dynamicznie oraz wydajność DNN. Hipernety to klasa sieci neuronowych\n",
    "które generują wagi innej sieci neuronowej zwanej siecią docelową/główną/podstawową, gdzie zarówno sieci są szkolone w sposób end-to-end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kluczowe cechy i zalety hipernetów, które oferują aplikacje w różnych ustawieniach problemów:\n",
    "\n",
    "* Soft weight sharing: Hipernetworks można trenować do generowania wag wielu DNN do rozwiązywania powiązanych zadań. Nazywa się to soft weight sharing, ponieważ w przeciwieństwie do hard weight sharing, który obejmuje wspólne warstwy między zadaniami (np. w multitasking), tutaj różne DNN są generowane przez wspólną hipernetę poprzez warunkowanie zadań. Jest to pomocne przy dzieleniu się informacjami między zadaniami i może być wykorzystywane do uczenia się transferowego lub dynamicznego udostępniania informacji.\n",
    "\n",
    "\n",
    "* Dynamic architectures: Hipernetworks mogą być używane do generowania wag sieci o architekturze dynamicznej, w której liczba warstw lub struktura sieci zmienia się podczas szkolenia lub wnioskowania. Może to być szczególnie przydatne w przypadku zadań, w których docelowa struktura sieci nie jest znana w czasie szkolenia.\n",
    "\n",
    "\n",
    "* Data-adaptive DNN: w przeciwieństwie do standardowego DNN, którego wagi są ustalane w czasie wnioskowania, HyperDNN można opracować w celu generowania sieci docelowej dostosowanej do potrzeb danych. W takich przypadkach hipernety są uwarunkowane danymi wejściowymi w celu dostosowania się do danych.\n",
    "\n",
    "\n",
    "* Uncertainty quantification: Hipernety mogą skutecznie trenować DNN ktore świadome o niepewności, wykorzystując techniki jak multiple inputs z rozkładu szumów lub włączenie przerywania w hipernetach sami. Generując wiele zestawów wag dla sieci głównej, hipernety tworzą zespół modeli, każdy z różnymi konfiguracjami parametrów. To podejście oparte na zespole pomaga w szacowaniu niepewność w przewidywaniach modelu, kluczowy aspekt dla zastosowań krytycznych dla bezpieczeństwa, takich jak opieka zdrowotna, gdzie niezbędna jest miara zaufania do prognoz\n",
    "\n",
    "\n",
    "* Parameter efficiency: DNN przeszkolony w hipernetach może mieć mniej wag niż\n",
    "standardowe DNNs, powodujące z kompresji wag. Może to być szczególnie przydatne, gdy\n",
    "cos diziala z ograniczonymi zasobami, ograniczonymi danymi lub w przypadku danych o dużych wymiarach i może skutkować szybszy trening niż zlykle DNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struktura i rola: Main Network to typowa sieć neuronowa, którą można zaprojektować do dowolnego zadania głębokiego uczenia, takiego jak klasyfikacja obrazu, przetwarzanie języka naturalnego lub inne zadania. Struktura tej sieci może się różnić w zależności od konkretnego zadania: może to być splotowa sieć neuronowa (CNN) do przetwarzania obrazu, powtarzająca się sieć neuronowa (RNN) do przetwarzania sekwencji danych i tak dalej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uczenie: w normalnej sytuacji wagi tej sieci są inicjowane losowo, a następnie optymalizowane w procesie uczenia się, aby rozwiązać dany problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypernetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struktura i rola: Hypernetwork to oddzielna sieć neuronowa, której celem jest generowanie wag dla sieci głównej. Ta sieć jest zwykle mniejsza i może być zaprojektowana tak, aby uwzględniać pewne cechy zadania, do którego szkolona jest sieć główna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interakcja z siecią główną: Hypernetwork pobiera dane wejściowe, które mogą zawierać informacje o stanie lub strukturze sieci głównej i na podstawie tych danych generuje wagi dla sieci głównej. Oznacza to, że wagi głównej sieci nie są bezpośrednio optymalizowane za pomocą standardowych metod szkoleniowych, takich jak propagacja wsteczna błędu, ale są generowane i zmieniane dynamicznie dzięki Hypernetwork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorisation of Hypernetworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует разделение на 5 критериев дезайна гиперсетей\n",
    "- input-based \n",
    "- output-based\n",
    "- variability of inputs \n",
    "- variability of outputs\n",
    "- architecture-based\n",
    "\n",
    "с помощью каждого из них можно можно подробнее понять работы конкретной гиперсети отвечая на вопросы:\n",
    "\n",
    "![categorisation.png](./imgs/categorisation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input-based hypernetworks\n",
    "Hypernetworks take a context vector as an input and generate weights of the target DNN as output. Depending on what\n",
    "context vector is used, we can have the following types of hypernetworks\n",
    "\n",
    "\n",
    "\n",
    "- Task-conditioned hypernetworks: These hypernetworks take task-specific information as input. The task information\n",
    "can be in the form of task identity/embedding, hyperparameters, architectures, or any other task-specific cues. The\n",
    "hypernetwork generates weights that are tailored to the specific task. This allows the hypernet to adapt its behavior\n",
    "accordingly and allows information sharing, through soft weight sharing of hypernets, among the tasks, resulting in\n",
    "better performance on the tasks\n",
    "\n",
    "- Data-conditioned hypernetworks: These hypernetworks are conditioned on the data that the target network is being\n",
    "trained on. The hypernetwork generates weights based on the characteristics of the input data. This enables the neural\n",
    "network to dynamically adjust its behavior based on the specific input pattern or features, leading to more flexible and\n",
    "adaptive models, and resulting in better generalization to unseen data.\n",
    "\n",
    "\n",
    "- Noise-conditioned hypernetworks: These hypernetworks are not conditioned on any input data or task cues, but rather\n",
    "on randomly sampled noise. This makes them more general-purpose and helps in predictive uncertainty quantification\n",
    "for DNNs, but it also means that they may not perform as well as task-conditioned or data-conditioned hypernetworks\n",
    "on multiple tasks or datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output-based Hypernetworks\n",
    "\n",
    "\n",
    "Based on the outputs of hypernets, i.e., weight generation strategy, we classify hypernetworks according to whether\n",
    "all weights are generated together or not. This classification of hypernetworks is important because it controls the\n",
    "scalability and complexity of the hypernetworks, as typically DNNs have a large number of weights, and producing all\n",
    "of them together can make the size of the last layer of hypernets large.\n",
    "\n",
    "\n",
    "\n",
    "- Generate Once: These hypernetworks generate weights of the entire target DNN altogether. This approach uses all\n",
    "the generated weights, and weights of each layer are generated together, unlike the other weight generation strategies.\n",
    "However, this weight generation approach is not suitable for large target networks because that can lead to complex\n",
    "hypernets\n",
    "\n",
    "\n",
    "\n",
    "- Generate Multiple: These hypernetworks have multiple heads for producing weights (sometimes referred to as\n",
    "split/multi-head hypernets) and this weight generation approach can complement the other approaches. This simplifies\n",
    "the complexity and reduces the number of weights required in the last layer of the hypernets by the number of head\n",
    "times. This approach does not need additional embeddings, and in general, uses all the generated weights, unlike component-wise and chunk-wise weight generation approaches where some weights remain unused.\n",
    "\n",
    "\n",
    "- Generate Chunk-wise: Chunk-wise hypernetworks generate weights of the target network in chunks. This can lead to\n",
    "not using some of the generated weights because the weights are generated as per the chunk size, which may not match\n",
    "the layer sizes. If the chunk size is smaller than the layer size, then all the weights of a layer may not be generated\n",
    "together. Moreover, these hypernets need additional embeddings to distinguish different chunks and to produce specific\n",
    "weights for the chunks. However, overall chunk-wise weight generation leads to reducing complexity and improving\n",
    "the scalability of hypernets.\n",
    "\n",
    "\n",
    "- Generate Component-wise: Component-wise weights generation strategy generates weights for each individual\n",
    "component (such as layer or channel) of the target model separately. This is helpful in generating specific weights\n",
    "because different layers or channels represent different features or patterns in the network. However, similar to the\n",
    "chunk-wise approach, component-wise hypernets need an embedding for each component to distinguish among different\n",
    "components and produce weights specific to that component. They also help to reduce the complexity and improve the\n",
    "scalability of hypernets. Since the weights are generated as per the size of the largest layer so this weight generation\n",
    "approach can lead to not using some of weights in smaller layers. This strategy can be seen as a special case of a\n",
    "chunk-wise weight generation approach, where one chunk is equal to the size of one component.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variability of Inputs\n",
    "\n",
    "We can categorize hypernets based on the variability of the inputs. We have two classes, static inputs and dynamic\n",
    "inputs, as discussed below.\n",
    "\n",
    "- Static Inputs: If the inputs are predefined and are fixed then the hypernet is called static with respect to the inputs. For\n",
    "example, multitasking [42] has fixed number of tasks leading to fixed number of inputs. It is to be noted that here fixed\n",
    "input only means fixed tasks identities, however hypernets can learn embeddings for different tasks.\n",
    "\n",
    "\n",
    "- Dynamic Inputs: If the inputs change and generally are dependent on data on which the target network is trained,\n",
    "then the hypernet is called dynamic with respect to the inputs. Dynamic inputs help hypernetworks to introduce a new\n",
    "level of adaptability by dynamically generating the weights of the target network. This dynamic weight generation\n",
    "enables hypernetworks to respond to input-dependent context and adjust their behavior accordingly. By generating\n",
    "network weights based on specific inputs, hypernetworks can capture intricate patterns and dependencies that may vary\n",
    "across different instances of data. This adaptability leads to enhanced model performance, especially in scenarios with\n",
    "complex and evolving data distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variability of Outputs\n",
    "\n",
    "When classifying hypernetworks based on the nature of the target network’s weights, we can categorize them into two\n",
    "types, static outputs or dynamic outputs, as discussed below.\n",
    "\n",
    "\n",
    "- Static Outputs: If weights of the target network are fixed in size, then the hypernet is called static with respect to the\n",
    "outputs. In this case, the target network is also static. For example, Pan et al. [49], Szatkowski et al. [69] produce static weights.\n",
    "\n",
    "\n",
    "- Dynamic Outputs: If weights of the target network are not fixed, i.e., the architecture varies in size, then the hypernet\n",
    "is called dynamic with respect to the outputs, and the target network is also a dynamic network as it can have different\n",
    "architecture depending on the input of the hypernet. The dynamic weights can be generated, mainly, in two situations,\n",
    "first when the hypernet architecture is dynamic, e.g., "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamism in Hypernetworks\n",
    "\n",
    "This is a super categorization of Subsection 3.3 and 3.4 into broader category based on the dynamism in inputs or\n",
    "outputs of the hypernets, as discussed below.\n",
    "\n",
    "- Static Hypernets: If input of a hypernet is fixed, i.e., predefined and number of weights produced by hypernet for\n",
    "the target network are fixed, i.e., the architecture is fixed, then the hypernet is called as a static hypernet. This kind of\n",
    "hypernets work with predefined inputs, e.g., task identities, which can be learned as embeddings, but the tasks being\n",
    "solved remain same\n",
    "\n",
    "- Dynamic Hypernets: If input of a hypernet is based on input of target network, i.e., input data, or number of weights\n",
    "produced by hypernet for the target network are variable, i.e., the architecture is dynamic, then the hypernet is called\n",
    "as a dynamic hypernet. F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of Hypernetworks\n",
    "\n",
    "In the categorization of hypernetworks based on their architectures, we can classify them into four major types:\n",
    "multi-layer perceptrons (MLPs), convolutional neural networks (CNNs), recurrent neural networks (RNNs), and\n",
    "attention-based networks, as given below.\n",
    "\n",
    "\n",
    "- MLPs: MLP based hypernetworks employ a dense and fully connected architecture, allowing every input neuron to\n",
    "connect with every output neuron. This architecture enables a comprehensive weight generation process by considering\n",
    "the entire input information, e.g., [14].\n",
    "\n",
    "\n",
    "- CNNs: CNN hypernetworks, on the other hand, leverage convolutional layers to capture local patterns and spatial\n",
    "information. These hypernetworks excel in tasks involving spatial data, such as an image or video analysis, by extracting\n",
    "features from the input and generating weights or parameters accordingly, e.g., Nirkin et al. [46] employed MLP to\n",
    "implement hypernets.\n",
    "\n",
    "\n",
    "- RNNs: RNN hypernetworks incorporate recurrent connections in their architecture, facilitating feedback loops and\n",
    "sequential information processing. They dynamically generate weights or parameters based on previous states or inputs,\n",
    "making them well-suited for tasks involving sequential data, such as natural language processing or time series analysis,\n",
    "e.g., Ha et al. [23] employed RNN to implement hypernets.\n",
    "\n",
    "\n",
    "- Attention Attention-based hypernetworks incorporate attention mechanisms [72] into their architecture. By selectively\n",
    "focusing on relevant input features, these hypernetworks generate weights for the target network, allowing them to\n",
    "capture long-range dependencies and improve the quality of generated outputs, e.g., Volk et al. [73] employed attention\n",
    "to implement hypernets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

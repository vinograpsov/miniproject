{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERNETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTIOIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hipernety lub w skrócie hipernety to Sieci neuronowe, które generują wagi dla innej sieci neuronowej, znanej jako sieć docelowa.\n",
    "(Pojawiły się jako potężna technika głębokiego uczenia, która pozwala na większą elastyczność, zdolność adaptacji, dynamikę, szybsze szkolenie, wymianę informacji i kompresję modelu itp.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Deep learning (DL) zrewolucjonizował dziedzinę sztucznej inteligencji umożliwiając niezwykłe postępy w różnych dzidzinach, w tym wizja komputerowa, przetwarzanie języka naturalnego, wnioskowanie przyczynowe, i reinforcement learningu itp.)\n",
    "\n",
    "\n",
    "(Jednak pomimo ich sukcesu standardowe DNN pozostają restrykcyjne w pewnych warunkach. Na przykład, gdy DNN jest pzetrenowany, jego wagi, a także architektura są stałe i wszelkie zmiany w wagach lub architekturze wymagają ponownego przetrenowania DNN) \n",
    "\n",
    "\n",
    "(Ten brak zdolności adaptacyjnych i dynamizmu ogranicza elastyczność\n",
    "DNN, co czyni je mniej odpowiednimi do scenariuszy, w których wymagane są dynamiczne korekty lub adaptacja danych)\n",
    "\n",
    "\n",
    "\n",
    "(Hypernetworks pojawiły się jako obiecujący paradygmat architektoniczny w celu zwiększenia elastyczności poprzez adaptacyjność danych i architektury dynamicznie oraz wydajność DNN. Hipernety to klasa sieci neuronowych\n",
    "które generują wagi innej sieci neuronowej zwanej siecią docelową/główną/podstawową, gdzie zarówno sieci są szkolone w sposób end-to-end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kluczowe cechy i zalety hipernetów, które oferują aplikacje w różnych ustawieniach problemów:\n",
    "\n",
    "* Soft weight sharing: Hipernetworks można trenować do generowania wag wielu DNN do rozwiązywania powiązanych zadań. Nazywa się to soft weight sharing, ponieważ w przeciwieństwie do hard weight sharing, który obejmuje wspólne warstwy między zadaniami (np. w multitasking), tutaj różne DNN są generowane przez wspólną hipernetę poprzez warunkowanie zadań. Jest to pomocne przy dzieleniu się informacjami między zadaniami i może być wykorzystywane do uczenia się transferowego lub dynamicznego udostępniania informacji.\n",
    "\n",
    "\n",
    "* Dynamic architectures: Hipernetworks mogą być używane do generowania wag sieci o architekturze dynamicznej, w której liczba warstw lub struktura sieci zmienia się podczas szkolenia lub wnioskowania. Może to być szczególnie przydatne w przypadku zadań, w których docelowa struktura sieci nie jest znana w czasie szkolenia.\n",
    "\n",
    "\n",
    "* Data-adaptive DNN: w przeciwieństwie do standardowego DNN, którego wagi są ustalane w czasie wnioskowania, HyperDNN można opracować w celu generowania sieci docelowej dostosowanej do potrzeb danych. W takich przypadkach hipernety są uwarunkowane danymi wejściowymi w celu dostosowania się do danych.\n",
    "\n",
    "\n",
    "* Uncertainty quantification: Hipernety mogą skutecznie trenować DNN ktore świadome o niepewności, wykorzystując techniki jak multiple inputs z rozkładu szumów lub włączenie przerywania w hipernetach sami. Generując wiele zestawów wag dla sieci głównej, hipernety tworzą zespół modeli, każdy z różnymi konfiguracjami parametrów. To podejście oparte na zespole pomaga w szacowaniu niepewność w przewidywaniach modelu, kluczowy aspekt dla zastosowań krytycznych dla bezpieczeństwa, takich jak opieka zdrowotna, gdzie niezbędna jest miara zaufania do prognoz\n",
    "\n",
    "\n",
    "* Parameter efficiency: DNN przeszkolony w hipernetach może mieć mniej wag niż\n",
    "standardowe DNNs, powodujące z kompresji wag. Może to być szczególnie przydatne, gdy\n",
    "cos diziala z ograniczonymi zasobami, ograniczonymi danymi lub w przypadku danych o dużych wymiarach i może skutkować szybszy trening niż zlykle DNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struktura i rola: Main Network to typowa sieć neuronowa, którą można zaprojektować do dowolnego zadania głębokiego uczenia, takiego jak klasyfikacja obrazu, przetwarzanie języka naturalnego lub inne zadania. Struktura tej sieci może się różnić w zależności od konkretnego zadania: może to być splotowa sieć neuronowa (CNN) do przetwarzania obrazu, powtarzająca się sieć neuronowa (RNN) do przetwarzania sekwencji danych i tak dalej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uczenie: w normalnej sytuacji wagi tej sieci są inicjowane losowo, a następnie optymalizowane w procesie uczenia się, aby rozwiązać dany problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypernetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struktura i rola: Hypernetwork to oddzielna sieć neuronowa, której celem jest generowanie wag dla sieci głównej. Ta sieć jest zwykle mniejsza i może być zaprojektowana tak, aby uwzględniać pewne cechy zadania, do którego szkolona jest sieć główna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interakcja z siecią główną: Hypernetwork pobiera dane wejściowe, które mogą zawierać informacje o stanie lub strukturze sieci głównej i na podstawie tych danych generuje wagi dla sieci głównej. Oznacza to, że wagi głównej sieci nie są bezpośrednio optymalizowane za pomocą standardowych metod szkoleniowych, takich jak propagacja wsteczna błędu, ale są generowane i zmieniane dynamicznie dzięki Hypernetwork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
